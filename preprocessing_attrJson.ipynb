{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.corpus\n",
    "import json\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ItemInfo_attrsJSON = pd.read_hdf(\"ItemInfo_test_attrsJSON.h\")\n",
    "ItemInfo_attrsJSON['attrsJSON'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1315205, 2)\n",
      "processed docs  0\n",
      "processed docs  100000\n",
      "processed docs  200000\n",
      "processed docs  300000\n",
      "processed docs  400000\n",
      "processed docs  500000\n",
      "processed docs  600000\n",
      "processed docs  700000\n",
      "processed docs  800000\n",
      "processed docs  900000\n",
      "processed docs  1000000\n",
      "processed docs  1100000\n",
      "processed docs  1200000\n",
      "processed docs  1300000\n"
     ]
    }
   ],
   "source": [
    "print(ItemInfo_attrsJSON.shape)\n",
    "\n",
    "# Create dicitonary with common attributes\n",
    "attributes_dictionary = {}\n",
    "\n",
    "for i, j in enumerate(ItemInfo_attrsJSON['attrsJSON']):\n",
    "    if i % 100000 == 0 :\n",
    "        print('processed docs ',i)\n",
    "#     print(i , json)\n",
    "#     print(\"=======\")\n",
    "    j = str(j)\n",
    "    if len(j) > 0:\n",
    "        for key in json.loads(j).keys():\n",
    "#             key = jsn.lower()\n",
    "#             key = regex.sub(r\"\\p{P}\", \"\", key)\n",
    "            if key not in attributes_dictionary:\n",
    "                attributes_dictionary[key] = 1\n",
    "            else:\n",
    "                attributes_dictionary[key] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed docs  0\n",
      "processed docs  100000\n",
      "processed docs  200000\n",
      "processed docs  300000\n",
      "processed docs  400000\n",
      "processed docs  500000\n",
      "processed docs  600000\n",
      "processed docs  700000\n",
      "processed docs  800000\n",
      "processed docs  900000\n",
      "processed docs  1000000\n",
      "processed docs  1100000\n",
      "processed docs  1200000\n",
      "processed docs  1300000\n",
      "storing\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def clearJson(jsn):\n",
    "    jsn = jsn.lower() # to lower case\n",
    "    jsn = regex.sub(r\"[^\\P{P}-]+\", \" \", jsn) # replace punctuiation by space\n",
    "    jsn = regex.sub(r\"\\s+\", \" \", jsn) # replace multiple spaces by one\n",
    "    return jsn\n",
    "\n",
    "def deleteAttrFromJson(jsn):\n",
    "    pattern_replace_pair_list = [\n",
    "        (r\"(?<=\\W|^)%s(?=\\W|$)\"%a, \"\") for a in attributes_dictionary.keys()\n",
    "    ]\n",
    "    \n",
    "    for pattern, replace in pattern_replace_pair_list:\n",
    "        jsn = regex.sub(pattern, replace, jsn)\n",
    "    return jsn\n",
    "\n",
    "def getOnlyAttrDescription(jsn):\n",
    "    jsn = deleteAttrFromJson(jsn)\n",
    "    jsn = clearJson(jsn)\n",
    "    return jsn\n",
    "\n",
    "def getProcessedJson(jsn):\n",
    "    jsn = clearJson(jsn)\n",
    "    return jsn\n",
    "\n",
    "def getOnlyAttrTitles(jsn):\n",
    "    attrTitles = \"\"\n",
    "    for attr in attributes_dictionary.keys():\n",
    "        if attr in jsn:\n",
    "            attrTitles += attr + \" \"\n",
    "    return attrTitles.lower()\n",
    "\n",
    "processed_attrsJSON = []\n",
    "processed_attrsJSON_titles = []\n",
    "processed_attrsJSON_description = []\n",
    "\n",
    "for i, j in enumerate(ItemInfo_attrsJSON['attrsJSON']):\n",
    "    if i % 100000 == 0 :\n",
    "        print('processed docs ',i)\n",
    "    jsn = str(j)\n",
    "    processed_attrsJSON.append(getProcessedJson(jsn).strip())\n",
    "    processed_attrsJSON_titles.append(getOnlyAttrTitles(jsn).strip())\n",
    "    processed_attrsJSON_description.append(getOnlyAttrDescription(jsn).strip())\n",
    "\n",
    "ItemInfo_attrsJSON.drop('attrsJSON', axis=1, inplace=True)\n",
    "ItemInfo_attrsJSON['attrsJSON'] = processed_attrsJSON\n",
    "ItemInfo_attrsJSON['attrsJSON_titles'] = processed_attrsJSON_titles\n",
    "ItemInfo_attrsJSON['attrsJSON_description'] = processed_attrsJSON_description\n",
    "print('storing')\n",
    "ItemInfo_attrsJSON.to_hdf(\"ItemInfo_test_attrsJSON_processed.h\", 'w')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del ItemInfo_attrsJSON\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
