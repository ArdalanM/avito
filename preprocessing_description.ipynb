{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.corpus\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseReplacer:\n",
    "    def __init__(self, pattern_replace_pair_list=[]):\n",
    "        self.pattern_replace_pair_list = pattern_replace_pair_list\n",
    "    def transform(self, text):\n",
    "        for pattern, replace in self.pattern_replace_pair_list:\n",
    "            try:\n",
    "                text = regex.sub(pattern, replace, text)\n",
    "            except:\n",
    "                pass\n",
    "        return regex.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "class LowerCaseConverter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    Traditional -> traditional\n",
    "    \"\"\"\n",
    "    def transform(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "\n",
    "class LetterLetterSplitter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    For letter and letter\n",
    "    /:\n",
    "    Cleaner/Conditioner -> Cleaner Conditioner\n",
    "    -:\n",
    "    Vinyl-Leather-Rubber -> Vinyl Leather Rubber\n",
    "    For digit and digit, we keep it as we will generate some features via math operations,\n",
    "    such as approximate height/width/area etc.\n",
    "    /:\n",
    "    3/4 -> 3/4\n",
    "    -:\n",
    "    1-1/4 -> 1-1/4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r\"([a-zа-я]+)[/\\-]([a-zа-я]+)\", r\"\\1 \\2\"),\n",
    "        ]\n",
    "\n",
    "class DigitLetterSplitter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    x:\n",
    "    1x1x1x1x1 -> 1 x 1 x 1 x 1 x 1\n",
    "    19.875x31.5x1 -> 19.875 x 31.5 x 1\n",
    "    -:\n",
    "    1-Gang -> 1 Gang\n",
    "    48-Light -> 48 Light\n",
    "    .:\n",
    "    includes a tile flange to further simplify installation.60 in. L x 36 in. W x 20 in. ->\n",
    "    includes a tile flange to further simplify installation. 60 in. L x 36 in. W x 20 in.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "#             [^a-zа-я0-9]\n",
    "            (r\"(\\d+)[\\.\\-]*([a-zа-я]+)\", r\"\\1 \\2\"),\n",
    "            (r\"([a-zа-я]+)[\\.\\-]*(\\d+)\", r\"\\1 \\2\"),\n",
    "        ]\n",
    "\n",
    "\n",
    "class DigitCommaDigitMerger(BaseReplacer):\n",
    "    \"\"\"\n",
    "    1,000,000 -> 1000000\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r\"(?<=\\d+),(?=000)\", r\"\"),\n",
    "            (r\"(?<=\\d+).(?=000)\", r\"\"),\n",
    "        ]\n",
    "        \n",
    "class NumberDigitMapper(BaseReplacer):\n",
    "    \"\"\"\n",
    "    один -> 1\n",
    "    два -> 2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        numbers = [\n",
    "            \"ноль\", \"один\",\"два\",\"три\",\"четыре\",\"пять\",\"шесть\",\"семь\",\"восемь\",\"девять\",\"десять\",\"одиннадцать\",\"двенадцать\",\"тринадцать\",\"четырнадцать\",\"пятнадцать\",\"шестнадцать\",\"семнадцать\",\"восемнадцать\",\"девятнадцать\",\"двадцать\",\"тридцать\",\"сорок\",\"пятьдесят\",\"шестьдесят\",\"семьдесят\",\"восемьдесят\",\"девяносто\",\"сто\",\"двести\",\"триста\",\"четыреста\",\"пятьсот\",\"шестьсот\",\"семьсот\",\"восемьсот\",\"девятьсот\"\n",
    "        ]\n",
    "        digits = [\n",
    "            0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900\n",
    "        ]\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r\"(?<=\\W|^)%s(?=\\W|$)\"%n, str(d)) for n,d in zip(numbers, digits)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords= frozenset(word \\\n",
    "                     for word in nltk.corpus.stopwords.words(\"russian\") \\\n",
    "                     if word!=\"не\")\n",
    "stemmer = SnowballStemmer('russian')\n",
    "engChars = [ord(char) for char in u\"cCyoOBaAKpPeE\"]\n",
    "rusChars = [ord(char) for char in u\"сСуоОВаАКрРеЕ\"]\n",
    "eng_rusTranslateTable = dict(zip(engChars, rusChars))\n",
    "rus_engTranslateTable = dict(zip(rusChars, engChars))\n",
    "\n",
    "def correctWord (w):\n",
    "    \"\"\" Corrects word by replacing characters with written similarly depending\n",
    "    on which language the word. \n",
    "    Fraudsters use this technique to avoid detection by anti-fraud algorithms.\n",
    "    \"\"\"\n",
    "    if len(re.findall(r\"[а-я]\",w))>len(re.findall(r\"[a-z]\",w)):\n",
    "        return w.translate(eng_rusTranslateTable)\n",
    "    else:\n",
    "        return w.translate(rus_engTranslateTable)\n",
    "\n",
    "def getWords(text, stemmRequired = True,\n",
    "             correctWordRequired = True,\n",
    "             excludeStopwordsRequired = True):\n",
    "    \"\"\" Splits the text into words, discards stop words and applies stemmer. \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str - initial string\n",
    "    stemmRequired : bool - flag whether stemming required\n",
    "    correctWordRequired : bool - flag whether correction of words required     \n",
    "    \"\"\"\n",
    "    cleanText = re.sub(r\"\\p{P}+\", \"\", text)\n",
    "    cleanText = cleanText.replace(\"+\", \" \")\n",
    "#     cleanText = text.replace(\",\", \" \").replace(\".\", \" \")\n",
    "#     cleanText = re.sub(u'[^a-zа-я0-9]', ' ', text.lower())\n",
    "    if correctWordRequired:\n",
    "        if excludeStopwordsRequired:\n",
    "            words = [correctWord(w) \\\n",
    "                    if not stemmRequired or re.search(\"[0-9a-z]\", w) \\\n",
    "                    else stemmer.stem(correctWord(w)) \\\n",
    "                    for w in cleanText.split() \\\n",
    "                    if w not in stopwords]\n",
    "        else:\n",
    "            words = [correctWord(w) \\\n",
    "                    if not stemmRequired or re.search(\"[0-9a-z]\", w) \\\n",
    "                    else stemmer.stem(correctWord(w)) \\\n",
    "                    for w in cleanText.split() \n",
    "                    ]\n",
    "    else:\n",
    "        if excludeStopwordsRequired:\n",
    "            words = [w \\\n",
    "                    if not stemmRequired or re.search(\"[0-9a-z]\", w) \\\n",
    "                    else stemmer.stem(w) \\\n",
    "                    for w in cleanText.split() \\\n",
    "                    if w not in stopwords]\n",
    "        else:\n",
    "            words = [w \\\n",
    "                    if not stemmRequired or re.search(\"[0-9a-z]\", w) \\\n",
    "                    else stemmer.stem(w) \\\n",
    "                    for w in cleanText.split()\n",
    "                    ]\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(text):    \n",
    "    pattern_replace_pair_list = [\n",
    "                # Remove single & double apostrophes\n",
    "                (\"[\\\"]+\", r\"\"),\n",
    "                (\"[\\']+\", r\"\"),\n",
    "                # Remove product codes (long words (>5 characters) that are all caps, numbers or mix pf both)\n",
    "                # don't use raw string format\n",
    "    #             (\"[ ]?\\\\b[0-9A-Z-]{5,}\\\\b\", \"\"),\n",
    "            ]\n",
    "\n",
    "    text = BaseReplacer(pattern_replace_pair_list).transform(text)\n",
    "#     print(text)\n",
    "    text = LowerCaseConverter().transform(text)\n",
    "#     print(text)\n",
    "    text = DigitLetterSplitter().transform(text)\n",
    "#     print(text)\n",
    "    text = DigitCommaDigitMerger().transform(text)\n",
    "#     print(text)\n",
    "    text = NumberDigitMapper().transform(text)\n",
    "#     print(text)\n",
    "    text = getWords(text)\n",
    "#     print(text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD test\n",
      "preprocessing text\n",
      "(3344613, 2)\n",
      "processed docs  0\n",
      "processed docs  10000\n",
      "processed docs  20000\n",
      "processed docs  30000\n",
      "processed docs  40000\n",
      "processed docs  50000\n",
      "processed docs  60000\n",
      "processed docs  70000\n",
      "processed docs  80000\n",
      "processed docs  90000\n",
      "processed docs  100000\n",
      "processed docs  110000\n",
      "processed docs  120000\n",
      "processed docs  130000\n",
      "processed docs  140000\n",
      "processed docs  150000\n",
      "processed docs  160000\n",
      "processed docs  170000\n",
      "processed docs  180000\n",
      "processed docs  190000\n",
      "processed docs  200000\n",
      "processed docs  210000\n",
      "processed docs  220000\n",
      "processed docs  230000\n",
      "processed docs  240000\n",
      "processed docs  250000\n",
      "processed docs  260000\n",
      "processed docs  270000\n",
      "processed docs  280000\n",
      "processed docs  290000\n",
      "processed docs  300000\n",
      "processed docs  310000\n",
      "processed docs  320000\n",
      "processed docs  330000\n",
      "processed docs  340000\n",
      "processed docs  350000\n",
      "processed docs  360000\n",
      "processed docs  370000\n",
      "processed docs  380000\n",
      "processed docs  390000\n",
      "processed docs  400000\n",
      "processed docs  410000\n",
      "processed docs  420000\n",
      "processed docs  430000\n",
      "processed docs  440000\n",
      "processed docs  450000\n",
      "processed docs  460000\n",
      "processed docs  470000\n",
      "processed docs  480000\n",
      "processed docs  490000\n",
      "processed docs  500000\n",
      "processed docs  510000\n",
      "processed docs  520000\n",
      "processed docs  530000\n",
      "processed docs  540000\n",
      "processed docs  550000\n",
      "processed docs  560000\n",
      "processed docs  570000\n",
      "processed docs  580000\n",
      "processed docs  590000\n",
      "processed docs  600000\n",
      "processed docs  610000\n",
      "processed docs  620000\n",
      "processed docs  630000\n",
      "processed docs  640000\n",
      "processed docs  650000\n",
      "processed docs  660000\n",
      "processed docs  670000\n",
      "processed docs  680000\n",
      "processed docs  690000\n",
      "processed docs  700000\n",
      "processed docs  710000\n",
      "processed docs  720000\n",
      "processed docs  730000\n",
      "processed docs  740000\n",
      "processed docs  750000\n",
      "processed docs  760000\n",
      "processed docs  770000\n",
      "processed docs  780000\n",
      "processed docs  790000\n",
      "processed docs  800000\n",
      "processed docs  810000\n",
      "processed docs  820000\n",
      "processed docs  830000\n",
      "processed docs  840000\n",
      "processed docs  850000\n",
      "processed docs  860000\n",
      "processed docs  870000\n",
      "processed docs  880000\n",
      "processed docs  890000\n",
      "processed docs  900000\n",
      "processed docs  910000\n",
      "processed docs  920000\n",
      "processed docs  930000\n",
      "processed docs  940000\n",
      "processed docs  950000\n",
      "processed docs  960000\n",
      "processed docs  970000\n",
      "processed docs  980000\n",
      "processed docs  990000\n",
      "processed docs  1000000\n",
      "processed docs  1010000\n",
      "processed docs  1020000\n",
      "processed docs  1030000\n",
      "processed docs  1040000\n",
      "processed docs  1050000\n",
      "processed docs  1060000\n",
      "processed docs  1070000\n",
      "processed docs  1080000\n",
      "processed docs  1090000\n",
      "processed docs  1100000\n",
      "processed docs  1110000\n",
      "processed docs  1120000\n",
      "processed docs  1130000\n",
      "processed docs  1140000\n",
      "processed docs  1150000\n",
      "processed docs  1160000\n",
      "processed docs  1170000\n",
      "processed docs  1180000\n",
      "processed docs  1190000\n",
      "processed docs  1200000\n",
      "processed docs  1210000\n",
      "processed docs  1220000\n",
      "processed docs  1230000\n",
      "processed docs  1240000\n",
      "processed docs  1250000\n",
      "processed docs  1260000\n",
      "processed docs  1270000\n",
      "processed docs  1280000\n",
      "processed docs  1290000\n",
      "processed docs  1300000\n",
      "processed docs  1310000\n",
      "processed docs  1320000\n",
      "processed docs  1330000\n",
      "processed docs  1340000\n",
      "processed docs  1350000\n",
      "processed docs  1360000\n",
      "processed docs  1370000\n",
      "processed docs  1380000\n",
      "processed docs  1390000\n",
      "processed docs  1400000\n",
      "processed docs  1410000\n",
      "processed docs  1420000\n",
      "processed docs  1430000\n",
      "processed docs  1440000\n",
      "processed docs  1450000\n",
      "processed docs  1460000\n",
      "processed docs  1470000\n",
      "processed docs  1480000\n",
      "processed docs  1490000\n",
      "processed docs  1500000\n",
      "processed docs  1510000\n",
      "processed docs  1520000\n",
      "processed docs  1530000\n",
      "processed docs  1540000\n",
      "processed docs  1550000\n",
      "processed docs  1560000\n",
      "processed docs  1570000\n",
      "processed docs  1580000\n",
      "processed docs  1590000\n",
      "processed docs  1600000\n",
      "processed docs  1610000\n",
      "processed docs  1620000\n",
      "processed docs  1630000\n",
      "processed docs  1640000\n",
      "processed docs  1650000\n",
      "processed docs  1660000\n",
      "processed docs  1670000\n",
      "processed docs  1680000\n",
      "processed docs  1690000\n",
      "processed docs  1700000\n",
      "processed docs  1710000\n",
      "processed docs  1720000\n",
      "processed docs  1730000\n",
      "processed docs  1740000\n",
      "processed docs  1750000\n",
      "processed docs  1760000\n",
      "processed docs  1770000\n",
      "processed docs  1780000\n",
      "processed docs  1790000\n",
      "processed docs  1800000\n",
      "processed docs  1810000\n",
      "processed docs  1820000\n",
      "processed docs  1830000\n",
      "processed docs  1840000\n",
      "processed docs  1850000\n",
      "processed docs  1860000\n",
      "processed docs  1870000\n",
      "processed docs  1880000\n",
      "processed docs  1890000\n",
      "processed docs  1900000\n",
      "processed docs  1910000\n",
      "processed docs  1920000\n",
      "processed docs  1930000\n",
      "processed docs  1940000\n",
      "processed docs  1950000\n",
      "processed docs  1960000\n",
      "processed docs  1970000\n",
      "processed docs  1980000\n",
      "processed docs  1990000\n",
      "processed docs  2000000\n",
      "processed docs  2010000\n",
      "processed docs  2020000\n",
      "processed docs  2030000\n",
      "processed docs  2040000\n",
      "processed docs  2050000\n",
      "processed docs  2060000\n",
      "processed docs  2070000\n",
      "processed docs  2080000\n",
      "processed docs  2090000\n",
      "processed docs  2100000\n",
      "processed docs  2110000\n",
      "processed docs  2120000\n",
      "processed docs  2130000\n",
      "processed docs  2140000\n",
      "processed docs  2150000\n",
      "processed docs  2160000\n",
      "processed docs  2170000\n",
      "processed docs  2180000\n",
      "processed docs  2190000\n",
      "processed docs  2200000\n",
      "processed docs  2210000\n",
      "processed docs  2220000\n",
      "processed docs  2230000\n",
      "processed docs  2240000\n",
      "processed docs  2250000\n",
      "processed docs  2260000\n",
      "processed docs  2270000\n",
      "processed docs  2280000\n",
      "processed docs  2290000\n",
      "processed docs  2300000\n",
      "processed docs  2310000\n",
      "processed docs  2320000\n",
      "processed docs  2330000\n",
      "processed docs  2340000\n",
      "processed docs  2350000\n",
      "processed docs  2360000\n",
      "processed docs  2370000\n",
      "processed docs  2380000\n",
      "processed docs  2390000\n",
      "processed docs  2400000\n",
      "processed docs  2410000\n",
      "processed docs  2420000\n",
      "processed docs  2430000\n",
      "processed docs  2440000\n",
      "processed docs  2450000\n",
      "processed docs  2460000\n",
      "processed docs  2470000\n",
      "processed docs  2480000\n",
      "processed docs  2490000\n",
      "processed docs  2500000\n",
      "processed docs  2510000\n",
      "processed docs  2520000\n",
      "processed docs  2530000\n",
      "processed docs  2540000\n",
      "processed docs  2550000\n",
      "processed docs  2560000\n",
      "processed docs  2570000\n",
      "processed docs  2580000\n",
      "processed docs  2590000\n",
      "processed docs  2600000\n",
      "processed docs  2610000\n",
      "processed docs  2620000\n",
      "processed docs  2630000\n",
      "processed docs  2640000\n",
      "processed docs  2650000\n",
      "processed docs  2660000\n",
      "processed docs  2670000\n",
      "processed docs  2680000\n",
      "processed docs  2690000\n",
      "processed docs  2700000\n",
      "processed docs  2710000\n",
      "processed docs  2720000\n",
      "processed docs  2730000\n",
      "processed docs  2740000\n",
      "processed docs  2750000\n",
      "processed docs  2760000\n",
      "processed docs  2770000\n",
      "processed docs  2780000\n",
      "processed docs  2790000\n",
      "processed docs  2800000\n",
      "processed docs  2810000\n",
      "processed docs  2820000\n",
      "processed docs  2830000\n",
      "processed docs  2840000\n",
      "processed docs  2850000\n",
      "processed docs  2860000\n",
      "processed docs  2870000\n",
      "processed docs  2880000\n",
      "processed docs  2890000\n",
      "processed docs  2900000\n",
      "processed docs  2910000\n",
      "processed docs  2920000\n",
      "processed docs  2930000\n",
      "processed docs  2940000\n",
      "processed docs  2950000\n",
      "processed docs  2960000\n",
      "processed docs  2970000\n",
      "processed docs  2980000\n",
      "processed docs  2990000\n",
      "processed docs  3000000\n",
      "processed docs  3010000\n",
      "processed docs  3020000\n",
      "processed docs  3030000\n",
      "processed docs  3040000\n",
      "processed docs  3050000\n",
      "processed docs  3060000\n",
      "processed docs  3070000\n",
      "processed docs  3080000\n",
      "processed docs  3090000\n",
      "processed docs  3100000\n",
      "processed docs  3110000\n",
      "processed docs  3120000\n",
      "processed docs  3130000\n",
      "processed docs  3140000\n",
      "processed docs  3150000\n",
      "processed docs  3160000\n",
      "processed docs  3170000\n",
      "processed docs  3180000\n",
      "processed docs  3190000\n",
      "processed docs  3200000\n",
      "processed docs  3210000\n",
      "processed docs  3220000\n",
      "processed docs  3230000\n",
      "processed docs  3240000\n",
      "processed docs  3250000\n",
      "processed docs  3260000\n",
      "processed docs  3270000\n",
      "processed docs  3280000\n",
      "processed docs  3290000\n",
      "processed docs  3300000\n",
      "processed docs  3310000\n",
      "processed docs  3320000\n",
      "processed docs  3330000\n",
      "processed docs  3340000\n",
      "storing\n",
      "done\n",
      "LOAD test\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/evgeny/env/P3/lib/python3.4/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1875\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1876\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'description'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-08cd4ca2372c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOAD test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mpd_itemInfo_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ItemInfo_test_title.h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mpd_itemInfo_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'preprocessing text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/evgeny/env/P3/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/evgeny/env/P3/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1999\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/evgeny/env/P3/lib/python3.4/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/evgeny/env/P3/lib/python3.4/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3225\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3226\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/evgeny/env/P3/lib/python3.4/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'description'"
     ]
    }
   ],
   "source": [
    "print('LOAD test')\n",
    "pd_itemInfo_train = pd.read_hdf(\"ItemInfo_train_description.h\")\n",
    "pd_itemInfo_train['description'].fillna(\"\", inplace=True)\n",
    "\n",
    "print('preprocessing text')\n",
    "processed_text = []\n",
    "\n",
    "print(pd_itemInfo_train.shape)\n",
    "for index, description in enumerate(pd_itemInfo_train['description']):\n",
    "    if index % 10000 == 0 : \n",
    "        print('processed docs ',index)\n",
    "    processed_text.append(preprocess(description))\n",
    "\n",
    "pd_itemInfo_train.drop('description', axis=1, inplace=True)\n",
    "pd_itemInfo_train['description'] = processed_text\n",
    "print('storing')\n",
    "\n",
    "pd_itemInfo_train.to_hdf(\"ItemInfo_train_description_processed.h\", 'w')\n",
    "print('done')\n",
    "\n",
    "del pd_itemInfo_train\n",
    "del processed_text\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD test\n",
      "preprocessing text\n",
      "(1315205, 2)\n",
      "processed docs  0\n",
      "processed docs  10000\n",
      "processed docs  20000\n",
      "processed docs  30000\n",
      "processed docs  40000\n",
      "processed docs  50000\n"
     ]
    }
   ],
   "source": [
    "print('LOAD test')\n",
    "pd_itemInfo_test = pd.read_hdf(\"ItemInfo_test_description.h\")\n",
    "pd_itemInfo_test['description'].fillna(\"\", inplace=True)\n",
    "\n",
    "print('preprocessing text')\n",
    "processed_text = []\n",
    "\n",
    "print(pd_itemInfo_test.shape)\n",
    "for index, description in enumerate(pd_itemInfo_test['description']):\n",
    "    if index % 10000 == 0 : \n",
    "        print('processed docs ',index)\n",
    "    processed_text.append(preprocess(description))\n",
    "\n",
    "pd_itemInfo_test.drop('description', axis=1, inplace=True)\n",
    "pd_itemInfo_test['description'] = processed_text\n",
    "print('storing')\n",
    "\n",
    "pd_itemInfo_test.to_hdf(\"ItemInfo_test_description_processed.h\", 'w')\n",
    "print('done')\n",
    "\n",
    "del pd_itemInfo_test\n",
    "del processed_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
